Tested values
1: ---------------------------------------------------------------------------------------------------------------------------------------------------------------
	1500 Iterations
	24 steps
	
	50 effort
	15 stifness
	0.9 damping
	FLOPFEST
	
2: ---------------------------------------------------------------------------------------------------------------------------------------------------------------
	8000 Iterations
	64 steps
	
	150 effort
	32 stifness
	0.9 damping
	Able to walk, but unsure about observations:

	/home/veg/IsaacLab/logs/rsl_rl/cerberus_rough/2025-03-31_20-13-11/model_3900.pt

	Total timesteps: 517079040
	Iteration time: 4.29s
	Total time: 17028.28s
	ETA: 17507.4s

	Error executing job with overrides: []
	Traceback (most recent call last):
	  File "/home/veg/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/utils/hydra.py", line 101, in hydra_main
	    func(env_cfg, agent_cfg, *args, **kwargs)
	  File "/home/veg/IsaacLab/scripts/reinforcement_learning/rsl_rl/train.py", line 176, in main
	    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)
	  File "/home/veg/env_isaaclab/lib/python3.10/site-packages/rsl_rl/runners/on_policy_runner.py", line 260, in learn
	    loss_dict = self.alg.update()
	  File "/home/veg/env_isaaclab/lib/python3.10/site-packages/rsl_rl/algorithms/ppo.py", line 258, in update
	    self.policy.act(obs_batch, masks=masks_batch, hidden_states=hid_states_batch[0])
	  File "/home/veg/env_isaaclab/lib/python3.10/site-packages/rsl_rl/modules/actor_critic.py", line 122, in act
	    return self.distribution.sample()
	  File "/home/veg/env_isaaclab/lib/python3.10/site-packages/torch/distributions/normal.py", line 73, in sample
	    return torch.normal(self.loc.expand(shape), self.scale.expand(shape))
	RuntimeError: normal expects all elements of std >= 0.0
	
	
	
	
	
	
3: ---------------------------------------------------------------------------------------------------------------------------------------------------------------
	Eyeballed camera position
	x: 0.45 z 0.075
	
	Added camera removed rays
	
	500 iterations 1024 envs for testing purposes to see if learn to walk... :)
	
	Looked wack
	
	Checkpoint: 2025-04-01_13-29-10


4: ---------------------------------------------------------------------------------------------------------------------------------------------------------------

	1000 iterations 2048 envs
	
	added camera settings similar to d435i
	Absolutely failed:

5: ---------------------------------------------------------------------------------------------------------------------------------------------------------------

		              Learning iteration 2093/4500                      

		               Computation: 29408 steps/s (collection: 4.363s, learning 0.094s)
		     Mean action noise std: 1.26
		  Mean value_function loss: 49391862405842913030132954300416.0000
		       Mean surrogate loss: -0.0032
		         Mean entropy loss: 25.1491
		               Mean reward: 22.79
		       Mean episode length: 951.87
	Episode_Reward/track_lin_vel_xy_exp: 1.2873
	Episode_Reward/track_ang_vel_z_exp: 0.6279
	       Episode_Reward/lin_vel_z_l2: -0.0532
	      Episode_Reward/ang_vel_xy_l2: -0.1081
	     Episode_Reward/dof_torques_l2: -0.3330
		 Episode_Reward/dof_acc_l2: -0.1332
	     Episode_Reward/action_rate_l2: -0.4923
	Episode_Reward/track_lin_vel_xy_yaw_frame_exp: 0.3358
	      Episode_Reward/feet_air_time: -0.0013
	Episode_Reward/flat_orientation_l2: 0.0000
	     Episode_Reward/dof_pos_limits: 0.0000
		 Curriculum/terrain_levels: 6.0699
	Metrics/base_velocity/error_vel_xy: 0.2588
	Metrics/base_velocity/error_vel_yaw: 0.2871
	      Episode_Termination/time_out: 2.3125
	  Episode_Termination/base_contact: 0.2969

		           Total timesteps: 274464768
		            Iteration time: 4.46s
		                Total time: 9424.55s
		                       ETA: 10833.3s



	Raytracing in front.	
	follow xy frame.

	/home/veg/IsaacLab/logs/rsl_rl/cerberus_rough/2025-04-01_21-45-49/model_2050.pt

	Error executing job with overrides: []
	Traceback (most recent call last):
	  File "/home/veg/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/utils/hydra.py", line 101, in hydra_main
	    func(env_cfg, agent_cfg, *args, **kwargs)
	  File "/home/veg/IsaacLab/scripts/reinforcement_learning/rsl_rl/train.py", line 176, in main
	    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)
	  File "/home/veg/env_isaaclab/lib/python3.10/site-packages/rsl_rl/runners/on_policy_runner.py", line 260, in learn
	    loss_dict = self.alg.update()
	  File "/home/veg/env_isaaclab/lib/python3.10/site-packages/rsl_rl/algorithms/ppo.py", line 258, in update
	    self.policy.act(obs_batch, masks=masks_batch, hidden_states=hid_states_batch[0])
	  File "/home/veg/env_isaaclab/lib/python3.10/site-packages/rsl_rl/modules/actor_critic.py", line 122, in act
	    return self.distribution.sample()
	  File "/home/veg/env_isaaclab/lib/python3.10/site-packages/torch/distributions/normal.py", line 73, in sample
	    return torch.normal(self.loc.expand(shape), self.scale.expand(shape))
	RuntimeError: normal expects all elements of std >= 0.0
	
6: ---------------------------------------------------------------------------------------------------------------------------------------------------------------


	################################################################################
                       Learning iteration 499/500                       

                       Computation: 29354 steps/s (collection: 4.369s, learning 0.096s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.0320
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 22.9220
                       Mean reward: -0.62
               Mean episode length: 51.51
Episode_Reward/track_lin_vel_xy_exp: 0.0000
Episode_Reward/track_ang_vel_z_exp: 0.0000
  Episode_Reward/track_heading_exp: 0.0334
       Episode_Reward/lin_vel_z_l2: -0.0133
      Episode_Reward/ang_vel_xy_l2: -0.0070
     Episode_Reward/dof_torques_l2: -0.0149
         Episode_Reward/dof_acc_l2: -0.0061
     Episode_Reward/action_rate_l2: -0.0225
      Episode_Reward/feet_air_time: -0.0001
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.0000
Metrics/base_velocity/error_vel_xy: 0.1040
Metrics/base_velocity/error_vel_yaw: 0.0909
      Episode_Termination/time_out: 0.1406
  Episode_Termination/base_contact: 34.3281
--------------------------------------------------------------------------------
                   Total timesteps: 65536000
                    Iteration time: 4.47s
                        Total time: 2417.39s
                               ETA: 4.8s


	Fell over
	
	Tried to implement heading:

	/home/veg/IsaacLab/logs/rsl_rl/cerberus_rough/2025-04-02_20-54-57/model_499.pt






-----------------------

Blind model



        self.rewards.flat_orientation_l2.weight = -5.0
        self.rewards.dof_torques_l2.weight = -2.5e-5
        self.rewards.feet_air_time.weight = 0.5
        # change terrain to flat
        self.scene.terrain.terrain_type = "plane"
        self.scene.terrain.terrain_generator = None
        # no height scan
        self.scene.height_scanner = None
        self.observations.policy.height_scan = None
        # no terrain curriculum
        self.curriculum.terrain_levels = None
        
        OLD
        joint_pos={
            ".*_hip_joint": math.pi/36,
            ".*_thigh_joint": -5*math.pi/18,
            ".*_calf_joint": 0,
        },
        
        
        
Changed the joint pos relative to the absolute joint pos.


